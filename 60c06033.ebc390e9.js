(window.webpackJsonp=window.webpackJsonp||[]).push([[22],{119:function(e,t,n){"use strict";n.d(t,"a",(function(){return b})),n.d(t,"b",(function(){return d}));var a=n(0),i=n.n(a);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function p(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var c=i.a.createContext({}),s=function(e){var t=i.a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},b=function(e){var t=s(e.components);return i.a.createElement(c.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return i.a.createElement(i.a.Fragment,{},t)}},m=i.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,o=e.parentName,c=p(e,["components","mdxType","originalType","parentName"]),b=s(n),m=a,d=b["".concat(o,".").concat(m)]||b[m]||u[m]||r;return n?i.a.createElement(d,l(l({ref:t},c),{},{components:n})):i.a.createElement(d,l({ref:t},c))}));function d(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,o=new Array(r);o[0]=m;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l.mdxType="string"==typeof e?e:a,o[1]=l;for(var c=2;c<r;c++)o[c]=n[c];return i.a.createElement.apply(null,o)}return i.a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},188:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/data5-f9494eb338300b8d2bb405410c74c307.png"},189:function(e,t,n){"use strict";n.r(t),t.default=n.p+"assets/images/data6-a243da17dab77ded5e330183fd54c47e.png"},91:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return o})),n.d(t,"metadata",(function(){return l})),n.d(t,"toc",(function(){return p})),n.d(t,"default",(function(){return s}));var a=n(3),i=n(7),r=(n(0),n(119)),o={id:"pipeline_implementation",title:"Pipeline Implementation"},l={unversionedId:"pipeline_implementation",id:"pipeline_implementation",isDocsHomePage:!1,title:"Pipeline Implementation",description:"Pipeline implementation",source:"@site/docs/pipeline_implementation.md",slug:"/pipeline_implementation",permalink:"/API-Playbook/pipeline_implementation",editUrl:"https://github.com/LBHackney-IT/API-Playbook/edit/master/docs/pipeline_implementation.md",version:"current",sidebar:"docs",previous:{title:"Setting Up DMS",permalink:"/API-Playbook/data_migration"},next:{title:"Create your first End Point",permalink:"/API-Playbook/first_end_point"}},p=[{value:"Pipeline implementation",id:"pipeline-implementation",children:[]},{value:"How to set up the data pipeline for a project",id:"how-to-set-up-the-data-pipeline-for-a-project",children:[]}],c={toc:p};function s(e){var t=e.components,o=Object(i.a)(e,["components"]);return Object(r.b)("wrapper",Object(a.a)({},c,o,{components:t,mdxType:"MDXLayout"}),Object(r.b)("h2",{id:"pipeline-implementation"},"Pipeline implementation"),Object(r.b)("p",null,"  ",Object(r.b)("strong",{parentName:"p"}," S3 ")),Object(r.b)("p",null,"  The source S3 bucket has been configured to invoke a Lambda function when a file has been uploaded with extension .csv"),Object(r.b)("p",null,Object(r.b)("img",{alt:"alt text",src:n(188).default})),Object(r.b)("p",null,"The configuration for the source S3 bucket is done using the pipeline\u2019s serverless implementation - ",Object(r.b)("u",null," no manual set up is required for events ")," ."),Object(r.b)("p",null,"  ",Object(r.b)("img",{alt:"alt text",src:n(189).default})),Object(r.b)("p",null,"  ",Object(r.b)("strong",{parentName:"p"}," Lambda ")),Object(r.b)("p",null,"   The lambda function implements the following:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Receives S3 notifications")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Retrieves bucket and file details from the notification")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Truncates the target table in the target database")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Makes use of AWS Postgres function that copies data from an csv file in S3 to Postgres to migrate the data")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Logs any exceptions and errors to Cloudwatch"),Object(r.b)("p",{parentName:"li"},Object(r.b)("strong",{parentName:"p"}," Note: The Postgres database and table to match the CSV format needs to be created separately. ")))),Object(r.b)("h2",{id:"how-to-set-up-the-data-pipeline-for-a-project"},"How to set up the data pipeline for a project"),Object(r.b)("p",null," A template repository has been created for the data pipeline code implementation:"),Object(r.b)("p",null,Object(r.b)("a",{parentName:"p",href:"https://github.com/LBHackney-IT/s3-to-postgres-data-pipeline"},"https://github.com/LBHackney-IT/s3-to-postgres-data-pipeline")),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"Create a repository for your pipeline by using the above template"),Object(r.b)("li",{parentName:"ol"},"Update the code by replacing the names of the existing pipeline to the name of your project\u2019s pipeline"),Object(r.b)("li",{parentName:"ol"},"Ensure you populate the specified environment variables in the README file of the repository"),Object(r.b)("li",{parentName:"ol"},"Deploy using serverless - this will deploy the Lambda and it will set up an existing S3 bucket with the event it needs to listen for.")),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"}," Notes ")),Object(r.b)("ol",null,Object(r.b)("li",{parentName:"ol"},"You need to create the S3 bucket separately and provide the name in the serverless.yml file of the pipeline repository."),Object(r.b)("li",{parentName:"ol"},"You need to create the Postgres separately and create the table that will be the \u201ctarget\u201d with the same columns as the ones expected to be present in the .csv that will be uploaded to S3")),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"  Matt notes to be tidied up: ")),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Add the extension to the database: CREATE EXTENSION IF NOT EXISTS aws_s3 CASCADE;"),Object(r.b)("li",{parentName:"ul"},"RDS needs permissions to access the S3 bucket - GetObject and ListBucket."),Object(r.b)("li",{parentName:"ul"},"Policy created and role added then added that role to the RDS instance\u2026.(Manually at the moment - how to automate?)")))}s.isMDXComponent=!0}}]);